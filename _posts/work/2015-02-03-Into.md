---
layout: post
title: ReIntroducing Into
tagline: Clean data migration
category : work
draft: true
tags : [scipy, Python, Programming, Blaze]
---
{% include JB/setup %}

**tl;dr `into` efficiently migrates data between a variety of formats.**

`into` efficiently migrates data between a variety of formats.
These formats include both in-memory data structures like the following:

    list, set, tuple, Iterator
    numpy.ndarray, pandas.DataFrame, dynd.array
    Streaming Sequences of any of the above

as well as persistent data living outside of Python like the following:

    CSV, JSON, line-delimited-JSON
    HDF5 (both standard and pandas formatting), BColz, SAS
    SQL databases (anything supported by SQLAlchemy), Mongo

It migrates data between any pair of these formats efficiently by using a
network of pairwise conversions.


How to use it
-------------

The `into` function takes two arguments, a source and a target.  It moves data
in the source to the target.  The source and target can take the following
forms

* Source:
    * A Python object holding data (e.g. `list`, `DataFrame`, `sqlalchemy.Table`)
    * A string URI that points to data (e.g. `'accounts.csv'` or `'postgresql:///hostname::tablename'`)
* Target:
    * A Python object on to which we want to *append* data
    * A type of the *new* object that we want to create with our data
    * A string URI that points to where we want to shove our data


**TODO: Make Table**


Examples
--------

    $ pip install into

{% highlight Python %}
>>> from into import into
{% endhighlight %}

Turn list into numpy array

{% highlight Python %}
>>> import numpy as np
>>> into(np.ndarray, [1, 2, 3])
array([1, 2, 3])
{% endhighlight %}

Load CSV file into Python list

{% highlight Python %}
>>> into(list, 'accounts.csv')
[(1, 'Alice', 100),
 (2, 'Bob', 200),
 (3, 'Charlie', 300),
 (4, 'Denis', 400),
 (5, 'Edith', 500)]
{% endhighlight %}

Translate CSV file into JSON

{% highlight Python %}
>>> into('accounts.json', 'accounts.csv')
{% endhighlight %}

    $ head accounts.json
    {"balance": 100, "id": 1, "name": "Alice"}
    {"balance": 200, "id": 2, "name": "Bob"}
    {"balance": 300, "id": 3, "name": "Charlie"}
    {"balance": 400, "id": 4, "name": "Denis"}
    {"balance": 500, "id": 5, "name": "Edith"}

Translate line-delimited JSON into a Pandas DataFrame

{% highlight Python %}
>>> import pandas as pd
>>> into(pd.DataFrame, 'accounts2.json')
   balance  id      name
0      100   1     Alice
1      200   2       Bob
2      300   3   Charlie
3      400   4     Denis
4      500   5     Edith
{% endhighlight %}



How does it work?
-----------------

<img src="{{BASE_PATH}}/images/star.png" align="right" width="400px">

This is challenging.  Robust and efficient conversions between any two pairs of
formats is fraught with special cases and bizarre libraries.  The common
solution is to convert through a common format like in-memory lists, dicts,
etc. (see [dat](http://dat-data.com/)) or through a serialization format like
ProtoBuf/Thrift.  These are excellent options and often what you want.
Sometimes however this can be slow, particularly when dealing with live
computational systems or with finicky storage solutions.

Consider for example, migrating between a `numpy.recarray` and a
`pandas.DataFrame`.  We can migrate this data very quickly in place.  The bytes
of data don't need to change, only the metadata surrounding them.  We don't
need to serialize or translate to intermediate Pure-Python objects.

Consider also migrating data from a CSV file to a PostgreSQL database.  Using
Python iterators through SQLAlchemy we rarely exceed migration speeds greater
than 2000 records per second.  However using direct loaders native to
PostgreSQL we can achieve speeds greater than 50000 records per second.  This
is the difference between an overnight job and a cup of coffee.  However this
requires that we're flexible enough to use special code in special situations.

*Expert pairwise interactions are often an order of magnitude faster than
generic solutions.*

Into is a network of these pairwise migrations.  We visualize that network
below:

<img src="https://raw.githubusercontent.com/ContinuumIO/into/master/doc/images/conversions.png">

Each node is a data format.  Each directed edge is a function that transforms
data between two formats.  A single call to `into` may traverse multiple edges
and multiple intermediate formats.  For example, we when migrate a CSV file to
a Mongo database we might take the following route:

* Load in to a `DataFrame` (`pandas.read_csv`)
* Convert to `np.recarray` (`DataFrame.to_records`)
* Then to a Python `Iterator` (`np.ndarray.tolist`)
* Finally to Mongo (`pymongo.Collection.insert`)

Alternatively we could write a special function that uses MongoDB's native CSV
loader and shortcut this entire process with a direct edge `CSV -> Mongo`.

To find the most efficient route we weight the edges of this network with
relative costs (measured ad-hoc.)  We use `networkx` to find the shortest path
before we start the migration.  If for some reason an edge fails (raises
`NotImplementedError`) we reroute.

You'll note that we color some nodes *red*.  These nodes can be *larger than
memory*.  When we migrate between two red nodes (both the input and output may
be larger than memory) then we limit our path to the red subgraph to ensure
that we don't blow up mid-migration.

This networked approach allows developers to write very specialized code for
special situations and know that that code will only be used in the right case.
This approach allows us to handle a very complex problem in an isolated and
separable manner.  The central dispatching system keeps us sane.


History
-------

I wrote about `into` [long ago]({{BASE_PATH}}/work/2014-09-01-Blaze-into/) in
connection to Blaze.  I then promptly shut up about it.  This was because the
old implementation (before this network approach) was difficult to
extend/maintain and wasn't ready for prime-time.

Into is now ready for prime-time.  It's also available independently from
Blaze, both via `conda` and via `pip`.  The major dependencies are NumPy,
Pandas, and NetworkX, though of course if you want to take advantage of some of
the higher performing formats, like HDF5, you'll need to install those
libraries as well.  For anyone reading my blog I suspect that it's a pretty
light-weight dependency.


How do I get started?
---------------------

You should download a recent version.

    $ pip install --upgrade into
    or
    $ conda install into --channel blaze

You then might want to go through the first half of
[this tutorial](https://github.com/ContinuumIO/blaze-tutorial#into)

Or just give it a shot without reading anything.  My hope is that the interface
is simple enough that you're able to pick it up naturally.  If you run in to
issues then I'd love to hear about them at `blaze-dev@continuum.io`
