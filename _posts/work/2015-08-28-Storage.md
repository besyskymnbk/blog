---
layout: post
title: A Weekend with Asyncio
category : work
draft: true
tags : [scipy, Python, Programming, dask, blaze]
---
{% include JB/setup %}

*This work is supported by [Continuum Analytics](http://continuum.io)
and the [XDATA Program](http://www.darpa.mil/program/XDATA)
as part of the [Blaze Project](http://blaze.pydata.org)*

**tl;dr: We discuss efficient techniques for on-disk storage of tabular
data.  We demonstrate value on the NYCTaxi dataset using dask.dataframe and a
small project, [Castra](https://github.com/blaze/castra).**


Larger than Memory Data and Disk I/O
------------------------------------

We analyze large datasets (10-100GB) on our laptop by extending memory with
disk.  Tools like [dask.array](http://dask.pydata.org/en/latest/array.html) and
[dask.dataframe](http://dask.pydata.org/en/latest/dataframe.html) make this
easier for array and tabular data.

Interaction times can improve significantly (from minutes to seconds) if we
choose to store our data on disk efficiently.  This is particularly important
for large data because we can no longer separately "load in our data" while we
get a coffee and then iterate rapidly on our dataset once it's comfortably
in memory.

*Larger-than-memory datasets force our data interaction loop to include the
hard drive.*


CSV is convenient but slow
--------------------------

CSV is great.  It's human readable, accessible by every tool (even Excel!), and
pretty simple.

It's also quite slow.  The `pandas.read_csv` parser maxes out at 100MB/s
on simple data.  This doesn't include any keyword arguments like datetime
parsing that might slow it down.  Consider the time to parse a 24GB dataset:

    24GB / (100MB/s) == 4 minutes

To interact with our data meaningfully we need to keep delays in the tens of
seconds; otherwise people get up and work on something else.  This improvement
from a few minutes to a few seconds is entirely possible if we choose better
formats.


Example with CSVs
-----------------

As an example lets play with the NYC Taxi dataset using
[dask.dataframe](http://http://dask.pydata.org/en/latest/dataframe.html) a
library that copies the Pandas API but operates in chunks off of disk.

copy code and plot from notebook


Principles to store tabular data
--------------------------------

*What makes a storage solution efficient for tabular data?*

A good solution for might have the following attributes:

1.  Binary
2.  Columnar
3.  Categorical support
4.  Compressed
5.  Indexed/Partitioned

We discuss each of these below.


### Binary

Consider the text '1.23' as it is stored in a CSV file and how it is stored as
a Python/C float in memory:

*  CSV:  `1.23`
*  Python float: `0x3f9d70a4`

These look *very different*.  When we load `1.23` from a CSV textfile
we need to translate it to `0x3f9d70a4`; this takes time.

A binary format stores our data on disk exactly how it will look in memory; we
store the bytes `0x3f9d70a4` directly on disk so that when we load data
from disk to memory no extra translation is necessary.  Our file is no longer
human readable but it's much much faster.

This gets more intense when we consider datetimes:

*  CSV: 2015-08-25 12:13:14
*  NumPy datetime representation: 1440529994000000  (as an integer)

Every time we parse a datetime we need to compute how many microseconds it has
been since the epoch.  This calculation needs to take into account things like
how many days in each month, and leap years.  This is slow.  A binary
representation would record the integer directly on disk (as `0x51e278694a680`)
so that we can load our datetimes directly into memory without calculation.

Within the Python ecosystem people often use the following binary stores

*  HDF5
*  BColz


### Columnar

Many analytic computations only require a few columns at a time, often only
one, e.g.

>>> df.passenger_counts.value_counts.compute()

Of our 24 GB we may only need 2GB.  *Columnar* storage means storing each
column separately from the others so that we can be read the relevant columns
without having to pass through irrelevant columns.

Our CSV example failed at this.  While we only wanted two columns,
pickup_datetime and pickup_longitude, we had to pass through all of our data to
collect the relevant pieces.  The pickup location data was mixed in with all
the rest.

Within the Python ecosystem people often use the following binary stores

*  BColz


### Categoricals

*Categoricals encode repetitive text columns (normally very expensive) as
integers (very very cheap) in a way that is invisible to the user.*

Consider the following (mostly text) columns of our NYC taxi dataset:

In [3]: df[['medallion', 'vendor_id', 'rate_code', 'store_and_fwd_flag']].head()
Out[3]:
medallion                           vendor_id  rate_code store_and_fwd_flag
0  89D227B655E5C82AECF13C3F540D4CF4       CMT          1                  N
1  0BD7C8F5BA12B88E0B67BED28BEA73D8       CMT          1                  N
2  0BD7C8F5BA12B88E0B67BED28BEA73D8       CMT          1                  N
3  DFD2202EE08F7A8DC9A57B02ACB81FE2       CMT          1                  N
4  DFD2202EE08F7A8DC9A57B02ACB81FE2       CMT          1                  N

Each of these columns represents elements of a small set:

*  Two vendor ids
*  Twenty one rate codes
*  Three store-and-forward flags (Y, N, missing)
*  and about 13000 taxi medallions.

And yet we store these elements in very large dtypes:

In [4]: df[['medallion', 'vendor_id', 'rate_code', 'store_and_fwd_flag']].dtypes
Out[4]:
medallion             object
vendor_id             object
rate_code              int64
store_and_fwd_flag    object
dtype: object

The object dtype used for strings in Pandas takes up a lot of memory and is
quite slow:

In [1]: import sys

In [2]: sys.getsizeof('CMT')  # bytes
Out[2]: 40

Categoricals replace the original column with a column of integers (of the
appropriate size) along with a small index mapping those integers to the
original values.  I've [written about categoricals
before](http://matthewrocklin.com/blog/work/2015/06/18/Categoricals/) so I
won't go into too much depth here.  Categoricals increase both storage and
computational efficiency by about 10x if you have text data that describes
elements in a category.


### Compression

After we've encoded everything well and separated our columns we find ourselves
limited by disk I/O read speeds.  Disk read bandwidths range from 100MB/s
(laptop spinning disk hard drive) to 2GB/s (RAID of SSDs).  This strongly
depends on how large our reads are and the numbers I've given above are for
large sequential reads (e.g. reading all of a 100MB file in one go.)
Performance significantly degrades for smaller reads but for analytic queries
we're often in the large sequential read case (hooray!)

We reduce disk read times through compression.  Consider the datetimes of the
NYC taxi dataset.  These values are repetitive and slowly changing; a perfect match for modern compression techniques.

In [32]: ind = df.index.compute()
In [33]: ind
Out[33]:
DatetimeIndex(['2013-01-01 00:00:00', '2013-01-01 00:00:00',
               '2013-01-01 00:00:00', '2013-01-01 00:00:00',
               '2013-01-01 00:00:00', '2013-01-01 00:00:00',
               '2013-01-01 00:00:00', '2013-01-01 00:00:00',
               '2013-01-01 00:00:00', '2013-01-01 00:00:00',
               ...
               '2013-12-31 23:59:42', '2013-12-31 23:59:47',
               '2013-12-31 23:59:48', '2013-12-31 23:59:49',
               '2013-12-31 23:59:50', '2013-12-31 23:59:51',
               '2013-12-31 23:59:54', '2013-12-31 23:59:55',
               '2013-12-31 23:59:57', '2013-12-31 23:59:57'],
               dtype='datetime64[ns]', name=u'pickup_datetime', length=169893985, freq=None, tz=None)

We can use a modern compression library, like `fastlz` or [blosc](http://blosc.org/) to compress this data at high speeds.

In [36]: import blosc

In [37]: %time compressed = blosc.compress_ptr(ind.values.ctypes.data, len(ind), ind.values.dtype.alignment, clevel=5)
CPU times: user 3.22 s, sys: 332 ms, total: 3.55 s
Wall time: 512 ms

In [40]: len(compressed) / ind.nbytes
Out[40]: 0.14296813539337488

In [41]: ind.nbytes / 0.512 / 1e9  # GB/s compress bandwidth
Out[41]: 2.654593515625

In [42]: %time _ = blosc.decompress(compressed)
CPU times: user 1.3 s, sys: 438 ms, total: 1.74 s
Wall time: 406 ms

In [43]: ind.nbytes / 0.406 / 1e9  # GB/s compress bandwidth
Out[43]: 3.3476647290640393

So we store 7x less bytes on disk (thus septupling our effective disk I/O) by adding on an extra 3GB/s delay.  If we're on a really nice Macbook pro hard drive (~600MB/s) then this is a clear win.

Note however that some data is more or less compressable than others.

In [44]: x = df.pickup_latitude.compute().values
In [45]: %time compressed = blosc.compress_ptr(x.ctypes.data, len(x), x.dtype.alignment, clevel=5)
CPU times: user 5.87 s, sys: 0 ns, total: 5.87 s
Wall time: 925 ms

In [46]: len(compressed) / x.nbytes
Out[46]: 0.7518617315969132

Here on floating point data we compress more slowly and get only a marginal
compression.  This may still be worth it (perhaps at lower compression
intensity) but definitely isn't a huge win.

Compressing optimally requires thought.  General rules of thumb include the
following:

*  Compress integer dtypes
*  Compress datetimes
*  If your data is slowly varying (e.g. sorted time series) then use a shuffle filter (default in blosc)
*  Don't bother much with floating point dtypes
*  Compress categoricals (which are just integer dtypes)

Finally, don't confuse "modern compression" with gzip or bz2.  These are both
very very slow.  If dealing with text data, consider
[snappy](https://github.com/google/snappy) (also available via blosc.)


### Indexing/Partitioning

One column usually dominates our queries.  In time-series data this is time.
For personal data this is the user ID.  Just as column stores let us avoid
reading irrelevant columns, partitioning our data along a preferred index
column lets us avoid reading irrelevant rows.  We might only want the data for
the last month and don't want to read several years' worth.  We might only need
the information for Alice and not Bob.

Traditional relational databases provide indexes on any number of columns or
sets of columns.  This is excellent if you are using a traditional relational
database.  Unfortunately the data structures to provide arbitrary indexes
don't mix well with some of the attributes discussed above and we're limited to
a single index that partitions our data into sorted blocks.


Castra
------

With these goals in mind we built [Castra](https://github.com/blaze/castra), a
binary partitioned compressed columnstore with support for categoricals.

Castra is easy to use with pandas and dask.dataframe.

TODO: Load nyctaxi data into castra


Castra was started by myself and Valentin Haenel (current maintainer of
[bloscpack](https://github.com/blosc/bloscpack/) and
[bcolz](http://bcolz.blosc.org/)) during an evening sprint following PyData
Berlin (Castra is only 400 lines long).  Several bugfixes and refactors were
followed up by Phil Cloud and Jim Crist.
