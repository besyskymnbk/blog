---
layout: post
title: Asynchronous Optimization Algorithms with Dask
tagline: Computations that evolve on partial results
draft: true
category: work
tags: [Programming, Python, scipy]
theme: twitter
---
{% include JB/setup %}

*This work is supported by [Continuum Analytics](http://continuum.io),
the [XDATA Program](http://www.darpa.mil/program/XDATA),
and the Data Driven Discovery Initiative from the [Moore
Foundation](https://www.moore.org/).*

Summary
-------

In a previous post [we built convex optimization algorithms with
Dask](http://matthewrocklin.com/blog/work/2017/03/22/dask-glm-1) that ran
efficiently on a distributed cluster and were important for a broad class of
statistical and machine learning algorithms.

We now extend that work by looking at *asynchronous algorithms*.  We show the
following:

1.  APIs within Dask to build asynchronous computations generally, not just for
    machine learning and optimization
2.  Reasons why asynchronous algorithms are valuable in machine learning
3.  A concrete asynchronous algorithm (Async ADMM) and its performance on the
    airlines dataset as compared to the synchronous alternative presented in
    the last blogpost.

This blogpost is co-authored by [Chris White](https://github.com/moody-marlin/)
(Capital One) who knows optimization and [Matthew
Rocklin](http://matthewrocklin.com/) (Continuum Analytics) who knows
distributed computing.

Asynchronous vs Blocking Algorithms
-----------------------------------

When we say *asynchronous* we contrast it against synchronous or blocking.

In a blocking algorithm you send out a bunch of work and then wait for the
result.  Dask's normal `.compute()` interface is blocking.  Consider the
following computation where we score a bunch of inputs in parallel and then
find the best:

```python
import dask

scores = [dask.delayed(score)(x) for x in L]  # many lazy calls to the score function
best = dask.delayed(max)(scores)
best = best.compute()  # Trigger all computation and wait until complete
```

This *blocks*.  We can't do anything while it runs.  If we're in a Jupyter
notebook we'll see a little asterisk telling us that we have to wait.

<img src="{{ BASE_PATH }}/images/jupyter-blocking-cell.png"
     width="80%"
     alt="A Jupyter notebook cell blocking on a dask computation">

In a non-blocking or asynchronous algorithm we send out work and track results
as they come in.  We are still able to run commands locally while our
computations run in the background (or on other computers in the cluster).
Dask has a variety of asynchronous APIs, but the simplest is probably the
[concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html)
API where we submit functions and then can wait and act on their return

```python
from dask.distributed import Client, as_completed
client = Client('scheduler-address:8786')

# Send out several computations
futures = [client.submit(score, x) for x in L]

# Find max as results arrive
best = 0
for future in as_completed(futures):
    score = future.result()
    if score > best:
        best = score
```

These two solutions are equivalent.  They do the same work and run in the same
amount of time.  The blocking ``dask.delayed`` solution is probably simpler to
write down but the non-blocking ``futures + as_completed`` solution lets us be
more *flexible*.

For example, if we get a score that is *good enough* then we might stop early.
If we find that certain kinds of values are giving better scores than others
then we might submit more computations around those values while cancelling
others, changing our computation during execution.

This ability to monitor and adapt a computation during execution is one reason
why people choose asynchronous algorithms.  In the case of optimization
algorithms we are doing a search process and frequently updating parameters.
If we are able to update those parameters more frequently then we may be able
to slightly improve every subsequently launched computation.  Asynchronous
algorithms enable increased flow of information around the cluster in
comparison to more lock-step batch-iterative algorithms.


Asynchronous ADMM
-----------------

In our [last blogpost](http://matthewrocklin.com/blog/work/2017/03/22/dask-glm-1)
we showed a simplified implementation of [Alternating Direction Method of
Multipliers](http://stanford.edu/~boyd/admm.html) (ADMM) with
[dask.delayed](http://dask.pydata.org/en/latest/delayed.html).  We saw that in
a distributed context it performed well when compared to a more traditional
distributed gradient descent.  This algorithm works by solving a small
optimization problem on every chunk of our data using our current parameter
estimates, bringing these back to the local process, combining them, and then
sending out new computation on updated parameters.

Now we update this algorithm to update asynchronously, so that we update our
parameters continuously as partial results come in.  Instead of sending out and
waiting on batches of results we consume and emit a constant stream of tasks
with slightly improved parameter estimates.
